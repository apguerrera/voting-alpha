#!/usr/bin/env python3

## LICENSE: MIT

import hashlib
import platform
import sys, os, json, logging, subprocess
import time
import urllib.parse
from datetime import datetime

logging.basicConfig(level=logging.INFO)

from time import sleep
from contextlib import suppress
from collections import defaultdict

from botocore.exceptions import ClientError
import boto3
from boto3.s3.transfer import S3Transfer

import click

def get_env(name, default):
    return os.environ.get(name, default)

S3_DEV_BUCKET = get_env("S3_DEV_BUCKET", "flux-public-assets")
STACK_TMPL = "./stack/flux-securevote-voting-stack.yaml"

TEST_STACK_NAME = "sv-testnet-stack"
TEST_SUBDOMAIN = get_env("TEST_SUBDOMAIN", "tnalpha2")
TEST_NAMEPREFIX = "sv-testnet"
OFFSET = ""

VOTING_DOMAIN = get_env("VOTING_DOMAIN", "flux.vote")
MAXS_SSH_KEY = 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCr1lcY0jTSmCARinvFJHelYGx2p+Ky0YxskSVj53ywYaLRN96w8WdN7rpCCosDQbd9KzvmKbBHHAlL8noEtARmxP4tKRvGGRyKawLLPm530CJRv4bSz03Iw2kz2V2fUWjA/RO2gNK9DCXTdDM67avv8oB/QvSobm1rSNKj6CcjTTJuQxGhJcXKrU/BZwugMIM3ELByyD6w8Jizm12JWGVgJTEIkgqgaNhmek2OLsw8+1hIC9EfhQunH9izzhEd2HoGYf3IJXlESnuRyhb7uOvOPysQDL8Hrt/Po6Wi3Lhlczy7q6wrlGQwO5/ORAfpYXGuk1lc3mTI/srgSgc38Vt'


def set_offset(offset):
    global TEST_STACK_NAME, TEST_NAMEPREFIX, TEST_SUBDOMAIN, OFFSET
    TEST_STACK_NAME += f'-{offset}'
    TEST_SUBDOMAIN += f'-{offset}'
    TEST_NAMEPREFIX += f'-{offset}'
    OFFSET = offset


cfn = None
SILENT = False


def echo(*args, sep: str = " ", **kwargs):
    if not SILENT:
        click.echo(sep.join(args), **kwargs)


def must_run(cmd, silent=False):
    logging.info("Running `%s`" % cmd)
    out = subprocess.STDOUT
    err = None
    if silent:
        out = subprocess.DEVNULL
        err = out
    if type(cmd) is list:
        to_run = cmd
    elif type(cmd) is str:
        to_run = cmd.split(' ')
    else:
        raise Exception("must_run only accepts str or list")
    exit_code = subprocess.check_call(to_run, stdout=out, stderr=err)
    logging.debug("Command `%s` exited w code %d" % (cmd, exit_code))
    if exit_code != 0:
        logging.error("Failed to run %s" % cmd)
        raise Exception("Failed to run required cmd: %s" % cmd)


class CmdRunner:
    def __init__(self, cmd_runner=must_run):
        self.cmds = []
        self.cmd_runner = cmd_runner

    def add(self, name, cmd):
        self.cmds.append((name, cmd))

    def run(self, cmd_runner=None):
        cmd_runner = self.cmd_runner if cmd_runner is None else cmd_runner
        for (n, cmd) in self.cmds:
            logging.info("Running ({name}) as cmd:\n\t{cmd}\n".format(name=n, cmd=cmd))
            cmd_runner(cmd)


def print_output(msg):
    print("\n### RESULT ###\n\n{}".format(msg))


def get_stack(stack_name):
    ss = list(filter(lambda s: s['StackName'] == stack_name, cfn.list_stacks()['StackSummaries']))
    if len(ss) > 0:
        return ss[0]
    return None


def is_in_prog(stack_name):
    s = get_stack(stack_name)
    return s and 'IN_PROGRESS' in s['StackStatus']


def stack_exists(stack_name):
    s = get_stack(stack_name)
    return s and s['StackStatus'] != 'DELETE_COMPLETE'


def stack_failed(stack_name):
    s = get_stack(stack_name)
    return s and ('FAILED' in s['StackStatus'] or 'ROLLBACK' in s['StackStatus'])


def await_in_prog(stack_name):
    if is_in_prog(stack_name):
        logging.info("Waiting for CFN deploy to complete before we trigger another.")
        stream_cfn_events(stack_name, heading_prefix="AWAITING")
        logging.info("Previous deploy completed, performing deploy...")


def delete_stack(stack_name):
    r = cfn.delete_stack(StackName=stack_name)
    while stack_exists(stack_name):
        time.sleep(0.1)
    return r


def stack_del_if_necessary(stack_name):
    if stack_exists(stack_name) and get_stack(stack_name)['StackStatus'] == 'ROLLBACK_COMPLETE':
        logging.warning("Deleting {} as it's in ROLLBACK_COMPLETE".format(stack_name))
        delete_stack(stack_name)


def create_stack(stack_name, obj_url, params_aws):
    extra = {} if params_aws is None else {'Parameters': params_aws}
    r = cfn.create_stack(
        StackName=stack_name,
        TemplateURL=obj_url,
        Capabilities=['CAPABILITY_NAMED_IAM', 'CAPABILITY_AUTO_EXPAND'],
        OnFailure="DELETE",
        TimeoutInMinutes=20,
        **extra
    )
    return r['StackId']


def update_stack(stack_name, obj_url, params_aws):
    extra = {} if params_aws is None else {'Parameters': params_aws}
    r = cfn.update_stack(
        StackName=stack_name,
        TemplateURL=obj_url,
        Capabilities=['CAPABILITY_NAMED_IAM', 'CAPABILITY_AUTO_EXPAND'],
        **extra
    )
    return r['StackId']


def create_or_update_stack(stack_name, obj_url, params):
    if stack_exists(stack_name):
        return update_stack(stack_name, obj_url, params)
    else:
        return create_stack(stack_name, obj_url, params)


def get_failed_msgs(stack_name):
    evts_after = get_stack(stack_name).get('LastUpdatedTime', None)

    try:
        evts = cfn.describe_stack_events(StackName=stack_name)['StackEvents']
    except:
        logging.warning("***** Failed to get stack events! *****")
        return []
    evts_notable = list(
        filter(lambda evt:
               ('FAILED' in evt['ResourceStatus'] or 'ROLLBACK_IN_PROGRESS' in evt['ResourceStatus']) and
               (evts_after is None or evt['Timestamp'] > evts_after), evts))
    evts_w_dupes = list([(evt['LogicalResourceId'], evt.get('ResourceStatusReason', evt)) for evt in evts_notable])
    resources_done = set()
    evts_final = []
    for (r_id, reason) in evts_w_dupes:
        if r_id not in resources_done:
            evts_final.append((r_id, reason))
            resources_done.add(r_id)
    return evts_final


def _cfn_stack_extract_resources_status(stack_name, recursive=False, update_start=None):
    try:
        evts = cfn.describe_stack_events(StackName=stack_name)['StackEvents']
    except ClientError as e:
        if "does not exist" in str(e):
            return {}, [], update_start
        else:
            raise e
    resources = {}
    _res_ord = []

    # find update_start first
    if update_start is None:
        for evt in evts:
            r_name = evt['LogicalResourceId']
            is_update_create_complete = evt['ResourceStatus'] in ['UPDATE_IN_PROGRESS', 'CREATE_IN_PROGRESS',
                                                                  'UPDATE_COMPLETE', 'CREATE_COMPLETE']
            if update_start is None and r_name == stack_name and is_update_create_complete:
                # just break when we get to this point since we don't want anything afterwards.
                update_start = evt['Timestamp']
                break

    for evt in evts:
        r_name = evt['LogicalResourceId']
        if r_name in resources or r_name in _res_ord or r_name == stack_name:
            continue
        resources[r_name] = {'status': evt['ResourceStatus'],
                             'ts': str(evt['Timestamp'].time()).split('.')[0],
                             'reason': evt.get('ResourceStatusReason', '<MISSING>'),
                             'type': evt.get('ResourceType', ""),
                             'is_old': False if update_start is None else evt['Timestamp'] < update_start
                             }
        _res_ord.insert(0, r_name)
        if recursive and evt['ResourceType'] == "AWS::CloudFormation::Stack" and r_name != stack_name:
            phys_id = evt['PhysicalResourceId']
            if phys_id != "":
                sub_name = phys_id.split('/')[1]
                sub_res, sub_res_ord, _meh = _cfn_stack_extract_resources_status(sub_name, recursive=recursive,
                                                                                 update_start=update_start)
                _res_ord = [f"{r_name}/{n}" for n in sub_res_ord] + _res_ord
                resources.update({f"{r_name}/{k}": v for k, v in sub_res.items()})

    used = set()
    res_ord = [r for r in _res_ord if r not in used and (used.add(r) or True)]

    return resources, res_ord, update_start


def stream_cfn_events(stack_name, heading_prefix=None):
    # echo(json.dumps(_cfn_stack_extract_resources_status(stack_name, recursive=True), indent=2))
    # return
    import curses

    _e = None

    def _run(stdscr):
        nonlocal _e
        stdscr.border()
        stdscr_height, stdscr_width = stdscr.getmaxyx()
        max_str_len = stdscr_width - 1
        text_win = curses.newwin(stdscr_height - 1, stdscr_width, 1, 0)

        def gen_heading():
            dt = datetime.utcnow()
            heading = f'CFN Status ({stack_name})'
            if heading_prefix:
                heading = f'[{heading_prefix}] {heading}'
            return f'{dt.strftime("%H:%M:%S")} {heading}'

        def gen_progress_rotate(i):
            return {
                0: "|",
                1: "/",
                2: "â€”",
                3: "\\"
            }[i % 4]

        curses.noecho()
        text_win.addstr(0, 0, gen_heading())
        text_win.refresh()
        loop_counter = 0

        try:
            while is_in_prog(stack_name):
                loop_counter += 1
                # echo("Stack is in progress...")
                try:
                    resources, _resources_order, _update_start = _cfn_stack_extract_resources_status(stack_name, recursive=True)
                except ClientError as e:
                    if 'does not exist' in str(e):
                        echo(f"ClientError: {str(e)}")
                        break
                    raise e

                text_win.clear()
                text_win.addstr(0, 0, gen_heading() + f" | update started: {_update_start.strftime('%H:%M:%S')}")
                l_num = 1
                # max_depth = max([s.count('/') for s in _resources_order])
                # last_depth = 0
                for r in _resources_order[::-1]:
                    if r in resources:
                        if l_num > stdscr_height - 2:
                            break
                        _r = resources[r]
                        try:
                            depth = r.count("/")
                            start_char = "â”œ"  # "â””" if depth != last_depth else
                            end_char = "â”¬" if "Stack" in _r['type'] else " "
                            prefix = " " if _r['is_old'] else \
                                (gen_progress_rotate(loop_counter) if "IN_PROGRESS" in _r['status'] else "âœ”")
                            prefix += "   " * (depth-1) + (f"{start_char}â”€â”€" if depth > 0 else "")
                            prefix += end_char

                            the_str = f"{_r['ts']} {prefix} {_r['status']} - {r}"
                            text_win.addstr(l_num, 0, the_str[:max_str_len])
                            if "FAILED" in _r['status']:
                                l_num += 1
                                text_win.addstr(l_num, 0, f"         {prefix} --> {_r['reason']}"[:max_str_len])
                            last_depth = depth
                        except Exception as e:
                            logging.error(f"Failed to stream status events - ({str(e)}) - console window too small?")
                            sys.exit(1)
                        l_num += 1
                text_win.refresh()
                time.sleep(2)

            text_win.addstr(1, 0, f"----------- STACK ({stack_name}) COMPLETED! ----------- In Progress: {is_in_prog(stack_name)}")
            text_win.refresh()
            time.sleep(1.5)
        except Exception as e:
            _e = e
        finally:
            curses.nocbreak()
            stdscr.keypad(False)
            curses.echo()
            curses.endwin()

    curses.wrapper(_run)
    # echo(f"----------- STACK ({stack_name}) COMPLETED! ----------- In Progress: {is_in_prog(stack_name)}")

    if _e is not None:
        echo(f"Error in stream events: {_e}")


def s3_upload(filepath, obj_key, public_read=True):
    s3 = boto3.client('s3')
    s3_tfer = S3Transfer(s3)
    extra_args = {'ACL': 'public-read'} if public_read else {}
    s3_tfer.upload_file(filepath, S3_DEV_BUCKET, obj_key, extra_args=extra_args)
    obj_url = "{}/{}/{}".format(s3.meta.endpoint_url, S3_DEV_BUCKET, obj_key)
    return obj_url


def dict_to_cfn_params(params):
    return [{'ParameterKey': k, 'ParameterValue': v} for k, v in params.items()]


def is_wsl():
    return os.path.exists('/bin/wslpath')


def docker_lambda(cmd, extra_args=''):
    wsl = is_wsl()
    if wsl:
        print("Detected WSL...")
    mnt_dir = '$PWD' if not wsl else '$(wslpath -w $PWD)'
    # sts = boto3.client('sts')
    # session_token = sts.get_session_token(DurationSeconds=900)['Credentials']
    # env_args = ' '.join([f"-e {k}={v}" for k,v in {
    #     'AWS_ACCESS_KEY_ID': session_token['AccessKeyId'],
    #     'AWS_SESSION_TOKEN': session_token['SessionToken'],
    #     'AWS_SECRET_ACCESS_KEY': session_token['SecretAccessKey'],
    # }.items()])
    env_args = ''
    return os.system(f'docker run --rm -t -v "{mnt_dir}":/var/task {env_args} {extra_args} lambci/lambda:build-python3.6 {cmd}')


@click.group()
@click.option("--debug/--no-debug", default=False)
@click.option("--offset", "-o", default="", type=click.STRING, help="use an offset for names to avoid conflicts.")
@click.option("--region", default="", type=click.STRING, help="specify an AWS region if need be")
def cli(debug, offset, region):
    global cfn
    if debug:
        logging.basicConfig(level=logging.DEBUG)
        logging.debug("Debug mode enabled")
    if offset:
        set_offset(offset)
    cfn_extras = {} if region == '' else {'region_name': region}
    cfn = boto3.client('cloudformation', **cfn_extras)


py_targets_raw = ["cr", "members", "app"]
py_targets = click.Choice(py_targets_raw + ['all'])


@cli.command(name='pip')
@click.argument('target', default="all", type=py_targets) #, help="The target python app to install deps for; with 'all' being all targets")
@click.argument('pkgs', nargs=-1)
@click.option('--no-system', type=click.BOOL, is_flag=True, default=False,
              help="do not include --system in pip install args")
@click.option('--pip-args', type=click.STRING, default='', help="extra args to pass to pip3 install")
@click.option('--use-docker', type=click.BOOL, is_flag=True, default=False,
              help="use the lambda-build docker container to install dependencies (required on MacOS and Windows")
def pip(target, pkgs, no_system, pip_args, use_docker):
    targets = py_targets_raw if target == "all" else [target]
    _pkgs = list(pkgs)
    target_paths = [{'cr': 'stack/cr/common', 'members': 'stack/app/members', 'app': 'stack/app/common'}[target] for target in targets]
    for target_path in target_paths:
        _pip(target_path, _pkgs, no_system, pip_args, use_docker)

def _pip(target_path, pkgs, no_system, pip_args, use_docker):
    curr_dir = os.getcwd()
    if os.path.exists(target_path):
        reqs_file = 'requirements.txt'
        os.chdir(target_path)
        try:
            with open(reqs_file, 'r') as f:
                reqs_old = f.read().split('\n')
        except FileNotFoundError as e:
            reqs_old = []
        reqs = list(reqs_old)
        for pkg in pkgs:
            if pkg in reqs_old:
                continue
            reqs.append(pkg)
        print('New requirements file lines:', reqs)
        with open(reqs_file, 'w+') as f:
            f.write('\n'.join(reqs))
        print('Platform:', platform.system())
        system = '' if no_system else '--system'
        _cmd_outer = '{}'
        if platform.system() == "Darwin":
            _cmd_outer = 'docker run --rm -v "$PWD":/var/task lambci/lambda:build-python3.6 {}'
            system = ''
        # _cmd = _cmd_outer.format(...origcmd here...)
        pip_cmd = f'pip3 install --target=deps -r {reqs_file} --upgrade {pip_args}'
        # _ec = docker_lambda(pip_cmd)
        _ec = os.system(pip_cmd)

        if _ec != 0:
           raise Exception("Failed to install packages")
        print_output(f"Installed packages {pkgs} / {reqs} to {target_path}")
    else:
        raise Exception("Target of {} is unknown".format(target_path))
    os.chdir(curr_dir)


@cli.command(name='test-chaincode')
def test_chaincode():
    test_cmd = 'bash -c "cd stack/cr/chaincode; ls -al && tail cfnwrapper.py && tail lib.py; python3 test/test_mk_contract.py"'
    docker_lambda(test_cmd)


@cli.command(name="stream-cfn")
@click.argument('stack-name', type=click.STRING, default='')
def cmd_stream_cfn(stack_name):
    if stack_name == '':
        stack_name = TEST_STACK_NAME
    stream_cfn_events(stack_name)


@cli.command(name='upload-func')
@click.argument('func-path')
@click.argument('func-name')
def upload_func(func_path, func_name):
    if func_path[-3:] != ".py":
        logging.error("Must target a python file to upload (whole dir gets uploaded tho)")
        sys.exit(1)
    root_dir = os.path.dirname(os.path.realpath(__file__))
    dir_path = os.path.dirname(func_path)
    os.chdir(dir_path)
    if os.path.exists(f"{root_dir}/tmp-func.zip"):
        must_run(f'rm -v {root_dir}/tmp-func.zip', silent=True)
    must_run(f'zip -r {root_dir}/tmp-func.zip ./', silent=True)
    os.chdir(root_dir)
    with open('./tmp-func.zip', 'rb') as f:
        zip_bytes = f.read()
    # obj_key = f'./tmp-func-{int(time.time())}.zip'
    # obj_url = s3_upload(obj_key, hashlib.sha256(zip_bytes).hexdigest() + ".zip")

    awslam = boto3.client('lambda')
    awslam.update_function_code(FunctionName=func_name, ZipFile=zip_bytes, Publish=True)

    print_output(f"Updated function {func_name}")


@cli.command(name='deploy-ns')
@click.argument('stack-path')
@click.argument('stack-name')
@click.option('--params', default='',
              help="(JSON Dict) Parameters to use. Default will use '{\"pOffset\": \"<value provided via --offset>\"}'")
def deploy_nestedstack(stack_path, stack_name, params):
    TMP_TMPL_FILE = "./tmp-nestedstack-dev.yaml"
    pkg_cmd = f"aws cloudformation package --template-file {stack_path} --s3-bucket {S3_DEV_BUCKET} --output-template-file {TMP_TMPL_FILE}"
    must_run(pkg_cmd, silent=True)

    filename = os.path.basename(stack_path)
    obj_key = f'./ns-dev/tmp-{int(time.time())}-{filename}'
    obj_url = s3_upload(TMP_TMPL_FILE, obj_key)

    if not params:
        prev_params = cfn.describe_stacks(StackName=stack_name)['Stacks'][0]['Parameters']
        cfn_params = [{'ParameterKey': p['ParameterKey'], 'UsePreviousValue': True} for p in prev_params]
        print(f"Using params: {json.dumps(cfn_params)}")
    else:
        json_params = json.loads(params)
        if type(json_params) is list:
            cfn_params = json_params
        else:
            cfn_params = dict_to_cfn_params(json_params if params else {'pOffset': OFFSET})
    sid = create_or_update_stack(stack_name, obj_url, cfn_params)
    logging.info(f"Stack SID: {sid}")

    stream_cfn_events(stack_name, heading_prefix="DEPLOY-NS-DEV")
    print_output(f"Stack deploy/create/update run for {stack_name} w sid: {sid}")

    if stack_failed(stack_name):
        msgs = ["{}: {}".format(a, b) for (a, b) in get_failed_msgs(stack_name)]
        print_output("Deploy failed. Msgs: \n\n{}".format('\n'.join(msgs)))


@cli.command(name='deploy-macros')
@click.argument('stack-name', default='sv-macros')
def deploy_macros(stack_name):
    TMP_TMPL_FILE = "tmp-macros.yaml"

    pkg_cmd = f"aws cloudformation package --template-file ./stack/sv-macros.yaml --s3-bucket {S3_DEV_BUCKET} --output-template-file {TMP_TMPL_FILE}"
    print("Package:", pkg_cmd)
    must_run(pkg_cmd, silent=True)

    obj_key = "sv-macros.yaml"
    s3 = boto3.client('s3')
    s3_tfer = S3Transfer(s3)
    s3_tfer.upload_file(TMP_TMPL_FILE, S3_DEV_BUCKET, obj_key, extra_args={'ACL': 'public-read'})
    obj_url = "{}/{}/{}".format(s3.meta.endpoint_url, S3_DEV_BUCKET, obj_key)

    params_aws = []

    if stack_exists(stack_name):
        sid = update_stack(stack_name, obj_url, params_aws)
    else:
        sid = create_stack(stack_name, obj_url, params_aws)

    stream_cfn_events(stack_name, heading_prefix="DEPLOY")


@cli.command(name='destroy')
@click.argument('stack-name', type=click.STRING, default='')
@click.option("--watch", default=False, type=click.BOOL, is_flag=True, help="Stream CFN events as it deploys.")
@click.pass_context
def destroy(ctx, watch, stack_name):
    if stack_name == '':
        stack_name = TEST_STACK_NAME

    if watch:
        stream_cfn_events(stack_name, heading_prefix="AWAIT")

    del_resp = cfn.delete_stack(StackName=stack_name)
    print(f"Del response: {del_resp}")

    if watch:
        stream_cfn_events(stack_name, heading_prefix="DELETE")

    print_output(f"Deleted: {del_resp}")


def _step_to_parameters(step: int):
    steps = [ {'pCreateNodes': 'false'}, {'pCreateChaincodeStack': 'false'}, {'pCreateMembersApp': 'false'}, {} ]
    return steps[step]


@cli.command(name='deploy')
@click.argument('stack-name', type=click.STRING, default='')
@click.option("--deploy/--no-deploy", default=True)
@click.option("--open-browser/--no-open-browser", default=False)
@click.option("--del-packaged-tmpl/--no-del-packaged-tmpl", default=True,
              help="Remove tmp-stack.yaml after we're done generating + uploading it")
@click.option("--watch", default=False, type=click.BOOL, is_flag=True, help="Stream CFN events as it deploys.")
@click.option("--ssh-key", default=MAXS_SSH_KEY, type=click.STRING,
              help="the ssh key with which to configure EC2 nodes.")
@click.option("--use-existing", default=False, is_flag=True, type=click.BOOL,
              help="use tmp-stack.yaml if it exists instead of running aws cfn package")
@click.option("--use-last-uploaded", default=False, is_flag=True, type=click.BOOL,
              help="do not package a new template, just redeploy the last one uploaded to S3.")
@click.option("--step", default=3, type=click.IntRange(0, 3), help="Proceed only to step N of the stack deployment. The stack will _progressively_ deploy more and more resources as you increase step. The default is to deploy everything.")
@click.pass_context
def deploy(ctx, stack_name, deploy, open_browser, del_packaged_tmpl, watch, ssh_key, use_existing,
                use_last_uploaded, step):
    if stack_name == '':
        stack_name = TEST_STACK_NAME

    if deploy:
        await_in_prog(stack_name)

    TMP_TMPL_FILE = 'tmp-stack.yaml'
    obj_key = "templates/" + STACK_TMPL.split('/')[-1]
    s3 = boto3.client('s3')
    s3_tfer = S3Transfer(s3)

    if not use_last_uploaded:
        if os.path.exists(TMP_TMPL_FILE) and use_existing:
            print(f"{TMP_TMPL_FILE} exists - skipping `aws cloudformation package`; "
                  f"use --disable-use-existing to disable this and rm the old tmp file.")
        else:
            must_run("aws cloudformation package --template-file {} --s3-bucket {} --output-template-file {}"
                     .format(STACK_TMPL, S3_DEV_BUCKET, TMP_TMPL_FILE), silent=True)
        # upload tmp file regardless
        s3_tfer.upload_file(TMP_TMPL_FILE, S3_DEV_BUCKET, obj_key, extra_args={'ACL': 'public-read'})

    obj_url = "{}/{}/{}".format(s3.meta.endpoint_url, S3_DEV_BUCKET, obj_key)
    print(f"Template URL: {obj_url}")

    params = {
        'NamePrefix': TEST_NAMEPREFIX,
        # 'HostedZone': 'Z2NVOFJHPZ9S2O',
        'HostedZoneDomain': VOTING_DOMAIN,
        'EC2InstanceType': 't2.small',
        'NumberOfEthereumConsensusNodes': '1',
        'NumberOfEthereumPublicNodes': '2',
        'Subdomain': TEST_SUBDOMAIN,
        'SSHKey': ssh_key,
        'AdminEmail': 'test-stack-flux-vote@xk.io',
        'pDeployMacros': 'false',
        # 'pDeployAcmAutovalidate': 'false',
        # 'Nonce': str(int(time.time()))
    }
    params.update(**_step_to_parameters(step))
    params_aws = dict_to_cfn_params(params)

    cfn_console_create_url = "https://console.aws.amazon.com/cloudformation/home?region={region}#/stacks/new?stackName={stack_name}&templateURL={tmpl_url}".format(
        obj_url, region=s3.meta.region_name, stack_name=stack_name, tmpl_url=urllib.parse.quote(obj_url, safe=''))

    def gen_cfn_evt_url(sid):
        return "https://ap-southeast-2.console.aws.amazon.com/cloudformation/home?region=ap-southeast-2#/stacks/{}/events".format(
            urllib.parse.quote(sid, safe=''))

    def gen_open_url_cmd(url):
        return "cmd.exe /c start {}".format(url)

    if deploy:
        await_in_prog(stack_name)
        stack_del_if_necessary(stack_name)
        if stack_exists(stack_name):
            sid = update_stack(stack_name, obj_url, params_aws)
        else:
            sid = create_stack(stack_name, obj_url, params_aws)
        cfn_evt_url = gen_cfn_evt_url(sid)
        if open_browser:
            os.system(gen_open_url_cmd(cfn_evt_url))
        if watch:
            stream_cfn_events(stack_name, heading_prefix="DEPLOY")
            _exists = stack_exists(stack_name)
            if not _exists or stack_failed(stack_name):
                if _exists:
                    msgs = ["{}: {}".format(a, b) for (a, b) in get_failed_msgs(stack_name)]
                    print_output("Deploy failed. Msgs: \n\n{}".format('\n'.join(msgs)))
                else:
                    print_output("Deploy failed and is deleted.")
            else:
                print_output("Deploy Success! {}".format(cfn_evt_url))
                if del_packaged_tmpl:
                    os.system("rm {}".format(TMP_TMPL_FILE))
        else:
            print_output("Initiated deploy of {}\n\nLink: {}".format(sid, cfn_evt_url))
            if del_packaged_tmpl:
                os.system("rm {}".format(TMP_TMPL_FILE))

    else:
        if open_browser:
            _to_run = gen_open_url_cmd(cfn_console_create_url)
            print("Opening CFN template via console. CMD: {}".format(_to_run))
            os.system(_to_run)
        print_output("""Uploaded {}
        Launch via CloudFormation: {}""".format(obj_url, cfn_console_create_url))


# monkey patch so usage output looks nicer
sys.argv[0] = './manage'
cli()
